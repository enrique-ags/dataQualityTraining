{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Caso 1: Verificar el archivo de ventas de la tienda Zara.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "HPLnRvea21p1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fuente de el archivo a Analizar:** Kaggle\n",
        "\n",
        "---\n",
        "\n",
        "**Dirección:** https://www.kaggle.com/datasets/michaelhakim/zara-sales-analysis\n",
        "\n",
        "---\n",
        "\n",
        "**Acerca del archivo:** This Zara sales dataset is a comprehensive resource for e-commerce analytics and retail performance analysis. It captures critical sales data over a defined period, offering granular insights into product sales trends within Zara stores.\n",
        "\n",
        "\n",
        "---\n",
        "**Columnas Disponibles:** ['Product ID','Product Position','Promotion', 'Product Category','Seasonal', 'Sales Volume', 'brand', 'url', 'sku', 'name', 'description', 'price', 'currency', 'scraped_at', 'terms', 'section']\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Herramientas a utilizar en este ejercicio:**\n",
        "\n",
        "1.   Google Colab  https://colab.research.google.com/\n",
        "2.   Pydeequ https://pydeequ.readthedocs.io/en/latest/README.html\n",
        "3.   Pyspark 3.4\n",
        "4.   pydeequ 1.2.0-spark-3.4\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "31XaX8Gf27DR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IWQPQvL83o-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Objetivos**\n",
        "Verificar la calidad del dataset de ventas de Zara utilizando PyDeequ y PySpark, asegurando la integridad, consistencia y validez de los datos para un análisis de ventas confiable.\n",
        "Ejercicios a desarrollar:\n",
        "# **Preparación del ambiente de desarrollo.**\n",
        "\n",
        "1.   Abrir la plataforma de colab\n",
        "2.   Subir el archivo de ventas Zara_Sales_Analysis.csv y Zara_Sales_Analysis_missing.csv. Ambos archivos contienen lo mismo, con excepción de que el que contiene la palabra missing le faltan algunos registros.\n",
        "3. Instalar Pyspark y Pydeequ en Google colab:\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vRFiYn7G3-zG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Instalar Java, PySpark y PyDeequ\n",
        "# Instalar Java Development Kit 8\n",
        "!apt-get update -qq > /dev/null\n",
        "!apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Instalar PySpark (la versión que prefieras, compatible con tu Java)\n",
        "!pip install pyspark==3.4.1 pydeequ\n",
        "\n",
        "# Establecer las variables de entorno para Java\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] += \":/usr/lib/jvm/java-8-openjdk-amd64/bin\" # Asegura que java esté en el PATH"
      ],
      "metadata": {
        "id": "ddgqKcim306O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Crear una variable con la versión de spark\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TPxC2_iK4tPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# indicate your Spark version, here we use Spark 3.5 with pydeequ 1.4.0\n",
        "os.environ[\"SPARK_VERSION\"] = '3.3'"
      ],
      "metadata": {
        "id": "DSvPzMnt40nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Inicializar spark junto con Pydeequ\n"
      ],
      "metadata": {
        "id": "P290hZJi5A-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Iniciar la Sesión de Spark\n",
        "\n",
        "import pydeequ\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Iniciar la sesión de Spark\n",
        "# Se añade la configuración para cargar las dependencias de Deequ.\n",
        "# Es importante establecer la versión de Spark que usarás,\n",
        "# aquí usamos Spark 3.4.1 que es compatible con pydeequ 1.2.0-spark-3.4\n",
        "spark = (SparkSession\n",
        "         .builder\n",
        "         .appName(\"PyDeequ Colab Example\")\n",
        "         .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
        "         .config(\"spark.jars.excludes\", \"net.soreine.java:io.deequ-core\") # Evita conflictos si ya está en el classpath\n",
        "         .config(\"spark.driver.memory\", \"4g\") # Aumenta la memoria del driver si trabajas con datos grandes\n",
        "         .config(\"spark.executor.memory\", \"4g\") # Aumenta la memoria del executor\n",
        "         .config(\"spark.sql.warehouse.dir\", \"file:///tmp/spark-warehouse\") # Directorio para el almacén de Spark\n",
        "         .enableHiveSupport() # Habilita el soporte para Hive, útil para metadatos\n",
        "         .getOrCreate())\n",
        "\n",
        "print(\"Sesión de Spark iniciada correctamente.\")"
      ],
      "metadata": {
        "id": "ii9MAaKT5AfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Verificar Sparkcontext este ejecutándose correctamente.\n"
      ],
      "metadata": {
        "id": "hnJxkT6OIw-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "id": "cQoG51yIIzwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicios a desarrollar parte 2\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "RztlC8PJI90k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargando el archivo de ventas de Zara en pyspark y ejecutando algunas validaciones básicas.\n",
        "\n",
        "\n",
        "1.  Leer el archivo de ventas con pyspark, el archivo es un tipo csv.\n",
        "2.   Imprimir los primeros 10 registros dentro del archivo.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i4gOWl6qJYMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read CSV with semicolon (;) delimiter\n",
        "df = spark.read.csv(\n",
        "   '/content/Zara_Sales_Analysis_missing.csv',\n",
        "   header=True,\n",
        "   inferSchema=True,\n",
        "   sep=';'  # or delimiter=';'\n",
        ")\n",
        "# Show the DataFrame\n",
        "df.show(10,truncate=False)"
      ],
      "metadata": {
        "id": "922LUG12I8_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 3. Imprimir el schema(incluido el tipo de dato)\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "feSSRiA6Ljlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "KVKXsFjBLov1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Imprimer el conteo de registros\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "In49YAH5MGX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imprimir el conteo de registros\n",
        "df.count()"
      ],
      "metadata": {
        "id": "wmeb4HEqI8La"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Revisar informacion estadistica con describe\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FBzjo7X6Nlxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "c74nvfuRIzPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imprimir estadísticas de columnas numéricas(Price and Sales Volume)\n",
        "df_numeric_columns = df.select(['Sales Volume','price'])\n",
        "df_numeric_columns.describe().show()"
      ],
      "metadata": {
        "id": "v6S3B2zINXPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ver valores unicos de la columna brand\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vkd2yXDJOFoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imprimir valores únicos para la columna Brand\n",
        "df.select('brand').distinct().show()"
      ],
      "metadata": {
        "id": "b0trzF2cL7Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usando Pydeequ\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "XQdJr5vGOTEU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Buscando valores nulls en nuestro dataset\n",
        "\n",
        "---\n",
        "El objetivo es buscar valores nulos dentro de cada columna\n"
      ],
      "metadata": {
        "id": "f7pacJzdOZUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydeequ.checks import *\n",
        "from pydeequ.verification import *\n",
        "\n",
        "check = Check(spark, CheckLevel.Warning, \"Completeness Check\")\n",
        "\n",
        "# Add completeness checks for all columns to the single check object\n",
        "for columna in df.columns:\n",
        "    check = check.hasCompleteness(columna, lambda completeness: completeness >= 1)\n",
        "\n",
        "\n",
        "verification_result = (\n",
        "    VerificationSuite(spark)\n",
        "    .onData(df)\n",
        "    .addCheck(check) # Add the single check object\n",
        "    .run()\n",
        ")\n",
        "\n",
        "# Muestra resultados (filtrando solo columnas que fallan)\n",
        "resultados = VerificationResult.checkResultsAsDataFrame(spark, verification_result)\n",
        "resultados.show(truncate=False)"
      ],
      "metadata": {
        "id": "f2zMIhj1OS0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validando la columna Brand\n",
        "\n",
        "\n",
        "---\n",
        "La columna brand solo debe de tener los valores Zara, otros valores son incorrectos\n"
      ],
      "metadata": {
        "id": "WmUAOZ8vT22j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydeequ.checks import *\n",
        "from pydeequ.verification import *\n",
        "\n",
        "check = Check(spark, CheckLevel.Error, \"Brand Validation Check\")\n",
        "\n",
        "# Check if 'brand' column ONLY contains 'Zara'\n",
        "check = check.satisfies(\n",
        "    \"brand = 'Zara'\",  # SQL condition (must evaluate to True)\n",
        "    \"Brand must be Zara\",  # Constraint description\n",
        "    lambda compliance: compliance >= 1.0  # 100% compliance required\n",
        ")\n",
        "\n",
        "# Run verification\n",
        "result = VerificationSuite(spark).onData(df).addCheck(check).run()\n",
        "\n",
        "# Show results\n",
        "VerificationResult.checkResultsAsDataFrame(spark, result).show(truncate=False)"
      ],
      "metadata": {
        "id": "UO8_ubkDUBXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 validaciones en un mismo VerificationSuite\n",
        "\n",
        "---\n",
        "1. Brand solo puede contener Zara O Levi\n",
        "2. Precio no puede ser null/Negativo\n",
        "3. Precio no puede ser igual a cero\n",
        "4. Precio debe de ser en dolares\n",
        "5. Product ID debe ser unico\n",
        "6. URL debe tener una direccion valida\n"
      ],
      "metadata": {
        "id": "u0CEWu9bV4cF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydeequ.checks import *\n",
        "from pydeequ.verification import *\n",
        "\n",
        "verification_result = (\n",
        "    VerificationSuite(spark)\n",
        "    .onData(df)\n",
        "    .addCheck(\n",
        "        Check(spark, CheckLevel.Error, \"Data Quality Checks\")\n",
        "        # 1. Brand must be \"Zara\" or \"Levi's\" (95% compliance allowed)\n",
        "        .hasCompleteness(\"brand\", lambda completeness: completeness >= 1)  # 100% completness\n",
        "        .satisfies(\n",
        "            \"brand IN ('Zara', 'Levi')\",\n",
        "            \"Brand must be Zara/Levi's\",\n",
        "            lambda compliance: compliance >= 1  # 95% of rows must comply\n",
        "        )\n",
        "\n",
        "        # 2. Price must be non-null (98%) and non-negative (100%)\n",
        "        .hasCompleteness(\"price\", lambda completeness: completeness >= 1)  # 100% non-null\n",
        "        .isNonNegative(\"price\")  # 100% compliance (no tolerance for negative prices)\n",
        "        .satisfies(\n",
        "        \"price != 0\",\n",
        "        \"Price must not be zero\",\n",
        "        lambda compliance: compliance >= 1  # 100% of rows must comply\n",
        "        )\n",
        "\n",
        "\n",
        "        # 3. Currency must be \"USD\" (99% compliance)\n",
        "        .hasPattern(\"currency\", \"^USD$\", lambda compliance: compliance >= 1)  # 100% exact matches\n",
        "\n",
        "          # 4. New check: No duplicates in 'id' (100% uniqueness)\n",
        "        .hasUniqueness([\"Product ID\"], lambda uniqueness: uniqueness >= 1)\n",
        "    )\n",
        "\n",
        "    .run()\n",
        ")\n",
        "\n",
        "# Show only failures\n",
        "result_df = VerificationResult.checkResultsAsDataFrame(spark, verification_result)\n",
        "result_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "5KhVVk9SV48f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# URL Valida\n",
        "\n",
        "---\n",
        "\n",
        "Validar la columna URL es valida"
      ],
      "metadata": {
        "id": "qdzzPLaNe3Ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydeequ.checks import *\n",
        "from pydeequ.verification import *\n",
        "\n",
        "check = Check(spark, CheckLevel.Error, \"URL Validation Check\")\n",
        "\n",
        "# Regex for basic URL validation (adjust as needed)\n",
        "url_regex = (\n",
        "    r\"^(https?:\\/\\/)?\"  # http:// or https:// (optional)\n",
        "    r\"([\\w\\-]+(\\.[\\w\\-]+)+)\"  # Domain (e.g., google.com)\n",
        "    r\"([\\w\\-\\._\\~\\:\\/\\?\\#\\[\\]\\@\\!\\$\\&'\\(\\)\\*\\+\\,\\;\\=]*)?\"  # Optional path/query\n",
        ")\n",
        "\n",
        "check = check.hasPattern(\n",
        "    \"url\",  # Replace with your column name\n",
        "    url_regex,\n",
        "    lambda compliance: compliance >= 1.0  # 100% compliance (adjust threshold)\n",
        ")\n",
        "\n",
        "result = VerificationSuite(spark).onData(df).addCheck(check).run()\n",
        "VerificationResult.checkResultsAsDataFrame(spark, result).show(truncate=False)"
      ],
      "metadata": {
        "id": "kHBhbt56cLGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checks Disponibles en PyDeequ\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Los checks se aplican a través de la clase Check en el módulo pydeequ.checks. Cada método define una regla de calidad de datos que se evalúa sobre una columna o un conjunto de columnas en el DataFrame. Aquí están los más importantes:\n",
        "**Completitud y Nulidad:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        ".hasCompleteness(column, assertion): Verifica el porcentaje de valores no nulos en una columna.\n",
        "Ejemplo: .hasCompleteness(\"price\", lambda x: x >= 0.98) (98% de los valores deben ser no nulos).\n",
        "\n",
        ".isComplete(column): Verifica que una columna esté 100% completa (sin valores nulos).\n",
        "`Ejemplo: .isComplete(\"id\").`\n",
        "\n",
        "**Unicidad:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        ".**isUnique**(column): Verifica que todos los valores en una columna sean únicos (100% de unicidad).\n",
        "Ejemplo: .isUnique(\"id\").\n",
        "\n",
        ".hasUniqueness(columns, assertion): Verifica el porcentaje de filas únicas en una o más columnas.\n",
        "Ejemplo: .hasUniqueness([\"id\", \"name\"], lambda x: x >= 0.95) (95% de las filas deben ser únicas).\n",
        "\n",
        ".isPrimaryKey(columns): Verifica que una o más columnas formen una clave primaria (valores únicos y no nulos).\n",
        "Ejemplo: .isPrimaryKey([\"id\"]).\n",
        "\n",
        "**Valores Permitidos y Restricciones:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        ".satisfies(column_condition, constraint_name, assertion): Evalúa una condición SQL personalizada sobre una columna.\n",
        "Ejemplo: .satisfies(\"price > 0\", \"Price must be positive\", lambda x: x >= 1) (100% de los valores deben cumplir).\n",
        "\n",
        ".isContainedIn(column, allowed_values): Verifica que los valores de una columna estén en una lista específica.\n",
        "Ejemplo: .isContainedIn(\"brand\", [\"Zara\", \"Levi's\"]) (todos los valores deben ser \"Zara\" o \"Levi's\").\n",
        "\n",
        ".hasPattern(column, pattern, assertion): Verifica que los valores de una columna cumplan con un patrón regex.\n",
        "Ejemplo: .hasPattern(\"currency\", \"^USD$\", lambda x: x >= 0.99) (99% deben ser \"USD\").\n",
        "\n",
        "**Restricciones Numéricas:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        ".isNonNegative(column): Verifica que todos los valores en una columna sean no negativos (≥ 0).\n",
        "Ejemplo: .isNonNegative(\"price\").\n",
        "\n",
        ".hasMin(column, assertion): Verifica que el valor mínimo de una columna cumpla con una condición.\n",
        "Ejemplo: .hasMin(\"price\", lambda x: x >= 0) (el valor mínimo debe ser ≥ 0).\n",
        "\n",
        ".hasMax(column, assertion): Verifica que el valor máximo cumpla con una condición.\n",
        "Ejemplo: .hasMax(\"price\", lambda x: x <= 1000) (el valor máximo debe ser ≤ 1000).\n",
        "\n",
        ".hasMean(column, assertion): Verifica que el promedio de una columna cumpla con una condición.\n",
        "Ejemplo: .hasMean(\"price\", lambda x: 50 <= x <= 100).\n",
        "\n",
        ".hasStandardDeviation(column, assertion): Verifica la desviación estándar de una columna.\n",
        "Ejemplo: .hasStandardDeviation(\"price\", lambda x: x <= 10).\n",
        "\n",
        "**Tamaño del DataFrame:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        ".hasSize(assertion): Verifica el número total de filas en el DataFrame.\n",
        "Ejemplo: .hasSize(lambda x: x >= 1000) (debe haber al menos 1000 filas).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Distribución y Estadísticas:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        ".hasApproxQuantile(column, quantile, assertion): Verifica que un cuantil aproximado cumpla con una condición.\n",
        "Ejemplo: .hasApproxQuantile(\"price\", 0.5, lambda x: x >= 50) (la mediana debe ser ≥ 50).\n",
        "\n",
        ".hasHistogramValues(column, assertion): Verifica la distribución de valores en una columna categórica.\n",
        "Ejemplo: .hasHistogramValues(\"category\", lambda x: x[\"shirts\"] >= 100) (al menos 100 filas con categoría \"shirts\").\n",
        "\n",
        "Correlación y Relaciones:\n",
        ".hasCorrelation(column1, column2, assertion): Verifica la correlación entre dos columnas.\n",
        "Ejemplo: .hasCorrelation(\"price\", \"quantity\", lambda x: abs(x) <= 0.5) (correlación absoluta ≤ 0.5).\n",
        "\n",
        "**Conteo Distinto:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        ".hasDistinctness(columns, assertion): Verifica el porcentaje de valores distintos en una o más columnas.\n",
        "Ejemplo: .hasDistinctness([\"brand\"], lambda x: x <= 0.1) (máximo 10% de valores distintos).\n",
        "\n",
        ".hasApproxCountDistinct(column, assertion): Verifica el conteo aproximado de valores distintos.\n",
        "Ejemplo: .hasApproxCountDistinct(\"brand\", lambda x: x <= 10) (máximo 10 valores distintos).\n",
        "\n",
        "Validaciones de Tipo de Datos:\n",
        ".hasDataType(column, data_type, assertion): Verifica el porcentaje de valores que cumplen con un tipo de datos específico (por ejemplo, Conformance.IntegerType).\n",
        "Ejemplo: .hasDataType(\"price\", Conformance.IntegerType, lambda x: x >= 0.9) (90% deben ser enteros).\n",
        "\n",
        "\n",
        "**Validaciones de Dependencia:**\n",
        "---\n",
        "\n",
        "\n",
        ".hasConditionalDependency(column, dependent_column, assertion): Verifica dependencias condicionales entre columnas.\n",
        "Ejemplo: Verificar que si brand = 'Zara', entonces price > 10.\n",
        "\n",
        "\n",
        "**Validaciones de Entropía:**\n",
        "---\n",
        ".hasEntropy(column, assertion): Verifica la entropía de una columna (medida de aleatoriedad).\n",
        "Ejemplo: .hasEntropy(\"category\", lambda x: x >= 1.0).\n",
        "\n",
        "**Validaciones de Aproximación:**\n",
        "---\n",
        "\n",
        ".hasApproxCount(column, assertion): Verifica un conteo aproximado de filas que cumplen una condición.\n",
        "Ejemplo: .hasApproxCount(\"price > 0\", lambda x: x >= 100).\n",
        "\n",
        "**Notas Importantes:**\n",
        "Niveles de Check: Los checks se definen con un nivel de severidad (CheckLevel.Error o CheckLevel.Warning). Si un check falla con Error, el proceso puede detenerse; con Warning, solo se registra.\n",
        "Ejemplo: Check(spark, CheckLevel.Error, \"Data Quality Checks\").\n",
        "\n",
        "**Assertions:** La mayoría de los checks aceptan una función lambda para definir el umbral de cumplimiento (por ejemplo, lambda x: x >= 0.95 para un 95% de cumplimiento).\n",
        "\n",
        "**Ejecución:** Los checks se ejecutan con .run() y los resultados se obtienen en un VerificationResult, que puede inspeccionarse para ver qué reglas pasaron o fallaron.\n",
        "\n",
        "**Múltiples Columnas:** Algunos checks, como .hasUniqueness o .isPrimaryKey, permiten pasar una lista de columnas para validar combinaciones.\n",
        "\n",
        "**Personalización:** Para reglas más complejas, .satisfies() permite usar expresiones SQL arbitrarias, lo que da gran flexibilidad.\n",
        "\n",
        "Ejemplo Completo:\n",
        "\n",
        "\n",
        "```\n",
        "python\n",
        "\n",
        "from pydeequ.checks import Check, CheckLevel\n",
        "from pydeequ.verification import VerificationSuite\n",
        "\n",
        "check = Check(spark, CheckLevel.Error, \"Data Quality Checks\") \\\n",
        "    .hasCompleteness(\"price\", lambda x: x >= 0.98) \\\n",
        "    .isNonNegative(\"price\") \\\n",
        "    .satisfies(\"price != 0\", \"Price must not be zero\", lambda x: x >= 1) \\\n",
        "    .isContainedIn(\"brand\", [\"Zara\", \"Levi's\"]) \\\n",
        "    .hasPattern(\"currency\", \"^USD$\", lambda x: x >= 0.99) \\\n",
        "    .hasSize(lambda x: x >= 1000)\n",
        "\n",
        "verification_result = VerificationSuite(spark) \\\n",
        "    .onData(df) \\\n",
        "    .addCheck(check) \\\n",
        "    .run()\n",
        "\n",
        "verification_result.checkResultsAsDataFrame(spark).show()\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wCpK3nXwZx-S"
      }
    }
  ]
}